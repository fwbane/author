{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import os\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Dropout, GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.optimizers import SGD\n",
    "import gensim\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from model_template import Model\n",
    "from glove_keras_cnn import GloveKerasCnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True  \n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.csv'\n",
    "df = pd.read_csv(filename, index_col='id')\n",
    "authors = list(df.author.unique())\n",
    "lookup = {a: _ for _, a in enumerate(authors)}\n",
    "y_numbers = [lookup[i] for i in df.author]\n",
    "y_vecs = []\n",
    "for y in y_numbers:\n",
    "    base_vec = np.zeros(num_classes, dtype='int')\n",
    "    base_vec[y] = 1\n",
    "    y_vecs.append(base_vec)\n",
    "df['y'] = y_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EAP'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EAP_test_string = \"\"\"Once upon a midnight dreary, while I pondered, weak and weary, \n",
    "Over many a quaint and curious volume of forgotten lore, \n",
    "While I nodded, nearly napping, suddenly there came a tapping, \n",
    "As of some one gently rapping, rapping at my chamber door.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPL_test_string = \"\"\"In this luminous Company I was tolerated more because of my Years \n",
    "than for my Wit or Learning; being no Match at all for the rest. My Friendship for the \n",
    "celebrated Monsieur Voltaire was ever a Cause of Annoyance to the Doctor; who was deeply \n",
    "orthodox, and who us'd to say of the French Philosopher.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWS_test_string = \"\"\"A few seconds ago they had all been active and healthy beings, \n",
    "so full of employment they could not afford to mend his calÃ¨che unless tempted by \n",
    "some extraordinary reward; now the men declared themselves cripples and invalids, the \n",
    "children were orphans, the women helpless widows, and they would all die of hunger if \n",
    "his Eccellenza did not bestow a few grani.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"/media/D/data/glove/\"\n",
    "GLOVE_W2V_FILE = \"glove.840B.300d.w2vformat.txt\"\n",
    "GLOVE_W2V_PATH = os.path.join(GLOVE_DIR, GLOVE_W2V_FILE)\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(GLOVE_W2V_PATH)\n",
    "wv = glove_model.wv\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "for sentence in [EAP_test_string, HPL_test_string, MWS_test_string]:\n",
    "    sample_vecs = []\n",
    "    for token in tokenizer.tokenize(sentence):\n",
    "        try:\n",
    "            sample_vecs.append(wv[token])\n",
    "        except KeyError:\n",
    "            # print(token, \"not in wv\")\n",
    "            pass\n",
    "    vectorized_data.append(sample_vecs)\n",
    "pickle.dump(vectorized_data, open(\"glove_vectorized_test_sentences\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = pickle.load(open(\"glove_vectorized_test_sentences\", \"rb\"))\n",
    "vectorized_data = pad_trunc(vectorized_data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.asarray(vectorized_data)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = np.reshape(test, (len(test), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = glove_keras_cnn()\n",
    "model = cnn.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sandbox to try out different CNN model structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove_keras_cnn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pickle.load(open(\"X-glove-encoding\", \"rb\"))\n",
    "y = pickle.load(open(\"y-glove-encoding\", \"rb\"))\n",
    "vectorized_query = pickle.load(open(\"glove_vectorized_test_sentences\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = GloveKerasCnn()\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, y, test_size=0.2, random_state=707)\n",
    "X_train = cnn.pad_trunc(X_train, maxlen)\n",
    "X_dev = cnn.pad_trunc(X_dev, maxlen)\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
    "Y_train = np.reshape(Y_train, (len(Y_train), num_classes))\n",
    "X_dev = np.reshape(X_dev, (len(X_dev), maxlen, embedding_dims))\n",
    "Y_dev = np.reshape(Y_dev, (len(Y_dev), num_classes))\n",
    "model = cnn.create()\n",
    "model = cnn.train(model, X_train, Y_train, X_dev, Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_query = cnn.pad_trunc(vectorized_query, maxlen)\n",
    "padded_query = np.asarray(padded_query)\n",
    "predictions = cnn.model.predict(padded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_filters_1 = 256\n",
    "stacked_filters_2 = 128\n",
    "stacked_kernel_size_1 = 5\n",
    "stacked_kernel_size_2 = 3\n",
    "stacked_hidden_dims_1 = 250\n",
    "stacked_hidden_dims_2 = 50\n",
    "stacked_epochs = 5\n",
    "\n",
    "class GloveKerasStackedCnn(GloveKerasCnn):\n",
    "    def __init__(self, wv=None):\n",
    "        GloveKerasCnn.__init__(self)\n",
    "        if wv:\n",
    "            self.embedding = wv\n",
    "        self.mname = \"stacked_cnn_model.json\"\n",
    "        self.wname = \"stacked_cnn_weights.h5\"\n",
    "\n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(64, 3, padding='valid', activation='relu', input_shape=(maxlen, embedding_dims)))\n",
    "        model.add(Conv1D(64, 3, activation='relu'))\n",
    "        model.add(MaxPooling1D(5))\n",
    "        model.add(Conv1D(128, 3, activation='relu'))\n",
    "        model.add(Conv1D(128, 3, activation='relu'))\n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, model, X_train, Y_train, X_dev, Y_dev):\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, epochs=stacked_epochs, validation_data=(X_dev, Y_dev))\n",
    "        return model\n",
    "\n",
    "    def predict(self, query, vectorize=False):\n",
    "        if not self.model:\n",
    "            print(\"No model available for prediction\")\n",
    "        if vectorize:\n",
    "            if isinstance(query, str):\n",
    "                query = [query]\n",
    "            vectorized_query = self.vectorize(query)\n",
    "            pickle.dump(vectorized_query, open(\"glove_vectorized_test_sentences\", \"wb\"))\n",
    "        else:\n",
    "            vectorized_query = query\n",
    "        vectorized_query = self.pad_trunc(vectorized_query, maxlen)\n",
    "        vectorized_query = np.asarray(vectorized_query)\n",
    "        vectorized_query = np.reshape(vectorized_query, (len(query), maxlen, embedding_dims)) # Should be redundant, to ensure compliance\n",
    "        predictions = self.model.predict(vectorized_query)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting test set\n",
      "padding\n",
      "reshaping data\n"
     ]
    }
   ],
   "source": [
    "cnn2 = GloveKerasStackedCnn()\n",
    "print(\"splitting test set\")\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, y, test_size=0.2, random_state=2684)\n",
    "print(\"padding\")\n",
    "X_train = cnn2.pad_trunc(X_train, maxlen)\n",
    "X_dev = cnn2.pad_trunc(X_dev, maxlen)\n",
    "print(\"reshaping data\")\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
    "Y_train = np.reshape(Y_train, (len(Y_train), num_classes))\n",
    "X_dev = np.reshape(X_dev, (len(X_dev), maxlen, embedding_dims))\n",
    "Y_dev = np.reshape(Y_dev, (len(Y_dev), num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "training model\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/5\n",
      "15663/15663 [==============================] - 5s 289us/step - loss: 0.8017 - acc: 0.6409 - val_loss: 0.6678 - val_acc: 0.7301\n",
      "Epoch 2/5\n",
      "15663/15663 [==============================] - 3s 222us/step - loss: 0.5905 - acc: 0.7592 - val_loss: 0.5537 - val_acc: 0.7812\n",
      "Epoch 3/5\n",
      "15663/15663 [==============================] - 3s 223us/step - loss: 0.4786 - acc: 0.8106 - val_loss: 0.5471 - val_acc: 0.7875\n",
      "Epoch 4/5\n",
      "15663/15663 [==============================] - 3s 223us/step - loss: 0.3783 - acc: 0.8509 - val_loss: 0.5865 - val_acc: 0.7809\n",
      "Epoch 5/5\n",
      "15663/15663 [==============================] - 4s 224us/step - loss: 0.3153 - acc: 0.8809 - val_loss: 0.5613 - val_acc: 0.7965\n"
     ]
    }
   ],
   "source": [
    "cnn2 = GloveKerasStackedCnn()\n",
    "print(\"creating model\")\n",
    "model = cnn2.create()\n",
    "print(\"training model\")\n",
    "model = cnn2.train(model, X_train, Y_train, X_dev, Y_dev)\n",
    "\n",
    "padded_query = cnn2.pad_trunc(vectorized_query, maxlen)\n",
    "padded_query = np.asarray(padded_query)\n",
    "predictions = cnn2.model.predict(padded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0235273e-01 6.9760913e-01 3.8170085e-05]\n",
      " [9.9995458e-01 4.5402543e-05 6.9700423e-09]\n",
      " [3.2605074e-04 2.5730924e-04 9.9941671e-01]]\n",
      "[1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)\n",
    "print(np.argmax(predictions, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2.save(model, save_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_filters_1 = 128\n",
    "double_filters_2 = 256\n",
    "double_kernel_size_1 = 5\n",
    "double_kernel_size_2 = 3\n",
    "double_hidden_dims_1 = 250\n",
    "double_hidden_dims_2 = 50\n",
    "double_epochs = 10\n",
    "\n",
    "class GloveKerasDoubleCnn(GloveKerasCnn):\n",
    "    def __init__(self, wv=None):\n",
    "        GloveKerasCnn.__init__(self)\n",
    "        if wv:\n",
    "            self.embedding = wv\n",
    "        self.mname = \"double_cnn_model.json\"\n",
    "        self.wname = \"double_cnn_weights.h5\"\n",
    "\n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv1D(double_filters_1, 3, padding='valid', activation='relu', input_shape=(maxlen, embedding_dims)))\n",
    "        model.add(MaxPooling1D(3))\n",
    "        model.add(Conv1D(double_filters_2, 3, activation='relu'))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('sigmoid'))\n",
    "        self.model = model\n",
    "        return model\n",
    "\n",
    "    def train(self, model, X_train, Y_train, X_dev, Y_dev):\n",
    "        adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "        model.fit(X_train, Y_train, batch_size=batch_size, epochs=double_epochs, validation_data=(X_dev, Y_dev))\n",
    "        return model\n",
    "\n",
    "    def predict(self, query, vectorize=False):\n",
    "        if not self.model:\n",
    "            print(\"No model available for prediction\")\n",
    "        if vectorize:\n",
    "            if isinstance(query, str):\n",
    "                query = [query]\n",
    "            vectorized_query = self.vectorize(query)\n",
    "            pickle.dump(vectorized_query, open(\"glove_vectorized_test_sentences\", \"wb\"))\n",
    "        else:\n",
    "            vectorized_query = query\n",
    "        vectorized_query = self.pad_trunc(vectorized_query, maxlen)\n",
    "        vectorized_query = np.asarray(vectorized_query)\n",
    "        vectorized_query = np.reshape(vectorized_query, (len(query), maxlen, embedding_dims)) # Should be redundant, to ensure compliance\n",
    "        predictions = self.model.predict(vectorized_query)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting test set\n",
      "padding\n",
      "reshaping data\n"
     ]
    }
   ],
   "source": [
    "cnn3 = GloveKerasDoubleCnn()\n",
    "print(\"splitting test set\")\n",
    "X_train, X_dev, Y_train, Y_dev = train_test_split(X, y, test_size=0.2, random_state=115)\n",
    "print(\"padding\")\n",
    "X_train = cnn3.pad_trunc(X_train, maxlen)\n",
    "X_dev = cnn3.pad_trunc(X_dev, maxlen)\n",
    "print(\"reshaping data\")\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
    "Y_train = np.reshape(Y_train, (len(Y_train), num_classes))\n",
    "X_dev = np.reshape(X_dev, (len(X_dev), maxlen, embedding_dims))\n",
    "Y_dev = np.reshape(Y_dev, (len(Y_dev), num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating model\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_41 (Conv1D)           (None, 98, 128)           115328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_42 (Conv1D)           (None, 30, 256)           98560     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 771       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 214,659\n",
      "Trainable params: 214,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "training model\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/10\n",
      "15663/15663 [==============================] - 4s 282us/step - loss: 0.9911 - acc: 0.5057 - val_loss: 0.8047 - val_acc: 0.6547\n",
      "Epoch 2/10\n",
      "15663/15663 [==============================] - 3s 215us/step - loss: 0.7485 - acc: 0.6799 - val_loss: 0.6866 - val_acc: 0.7063\n",
      "Epoch 3/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.6585 - acc: 0.7234 - val_loss: 0.6420 - val_acc: 0.7303\n",
      "Epoch 4/10\n",
      "15663/15663 [==============================] - 3s 218us/step - loss: 0.6020 - acc: 0.7518 - val_loss: 0.6182 - val_acc: 0.7337\n",
      "Epoch 5/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.5491 - acc: 0.7752 - val_loss: 0.5899 - val_acc: 0.7495\n",
      "Epoch 6/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.5113 - acc: 0.7972 - val_loss: 0.5699 - val_acc: 0.7648\n",
      "Epoch 7/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.4701 - acc: 0.8156 - val_loss: 0.5633 - val_acc: 0.7663\n",
      "Epoch 8/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.4345 - acc: 0.8325 - val_loss: 0.5663 - val_acc: 0.7612\n",
      "Epoch 9/10\n",
      "15663/15663 [==============================] - 3s 216us/step - loss: 0.4026 - acc: 0.8470 - val_loss: 0.5452 - val_acc: 0.7743\n",
      "Epoch 10/10\n",
      "15663/15663 [==============================] - 3s 217us/step - loss: 0.3633 - acc: 0.8659 - val_loss: 0.5585 - val_acc: 0.7707\n",
      "[[3.0481815e-03 8.0779791e-03 7.1585178e-05]\n",
      " [1.3100237e-02 1.2767911e-03 4.3421984e-05]\n",
      " [2.2116303e-04 2.7197301e-03 1.5079094e-03]]\n"
     ]
    }
   ],
   "source": [
    "cnn3 = GloveKerasDoubleCnn()\n",
    "print(\"creating model\")\n",
    "model = cnn3.create()\n",
    "print(\"training model\")\n",
    "model = cnn3.train(model, X_train, Y_train, X_dev, Y_dev)\n",
    "\n",
    "padded_query = cnn3.pad_trunc(vectorized_query, maxlen)\n",
    "padded_query = np.asarray(padded_query)\n",
    "predictions = cnn3.model.predict(padded_query)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_query = np.reshape(padded_query, (3, maxlen, embedding_dims)) # Should be redundant, to ensure compliance\n",
    "predictions = cnn3.model.predict(padded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0883083e-03, 6.3081980e-03, 9.7662210e-05],\n",
       "       [1.3642669e-02, 2.6145577e-04, 1.0693073e-04],\n",
       "       [4.2319298e-05, 1.8084049e-03, 3.3995770e-03]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3.save(model, save_weights=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
