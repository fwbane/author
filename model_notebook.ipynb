{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import os\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv1D, Dropout, GlobalMaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "import gensim\n",
    "from keras.models import model_from_json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from model_template import Model\n",
    "from glove_keras_cnn import glove_keras_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True  \n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 4\n",
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    new_data = []\n",
    "    # Create a vector of 0s the length of our word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    for sample in data:\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            # Append the appropriate number 0 vectors to the list\n",
    "            additional_elems = maxlen - len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'train.csv'\n",
    "df = pd.read_csv(filename, index_col='id')\n",
    "authors = list(df.author.unique())\n",
    "lookup = {a: _ for _, a in enumerate(authors)}\n",
    "y_numbers = [lookup[i] for i in df.author]\n",
    "y_vecs = []\n",
    "for y in y_numbers:\n",
    "    base_vec = np.zeros(num_classes, dtype='int')\n",
    "    base_vec[y] = 1\n",
    "    y_vecs.append(base_vec)\n",
    "df['y'] = y_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EAP'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EAP_test_string = \"\"\"Once upon a midnight dreary, while I pondered, weak and weary, \n",
    "Over many a quaint and curious volume of forgotten lore, \n",
    "While I nodded, nearly napping, suddenly there came a tapping, \n",
    "As of some one gently rapping, rapping at my chamber door.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPL_test_string = \"\"\"In this luminous Company I was tolerated more because of my Years \n",
    "than for my Wit or Learning; being no Match at all for the rest. My Friendship for the \n",
    "celebrated Monsieur Voltaire was ever a Cause of Annoyance to the Doctor; who was deeply \n",
    "orthodox, and who us'd to say of the French Philosopher.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MWS_test_string = \"\"\"A few seconds ago they had all been active and healthy beings, \n",
    "so full of employment they could not afford to mend his calÃ¨che unless tempted by \n",
    "some extraordinary reward; now the men declared themselves cripples and invalids, the \n",
    "children were orphans, the women helpless widows, and they would all die of hunger if \n",
    "his Eccellenza did not bestow a few grani.\"\"\".replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIR = \"/media/D/data/glove/\"\n",
    "GLOVE_W2V_FILE = \"glove.840B.300d.w2vformat.txt\"\n",
    "GLOVE_W2V_PATH = os.path.join(GLOVE_DIR, GLOVE_W2V_FILE)\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format(GLOVE_W2V_PATH)\n",
    "wv = glove_model.wv\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = []\n",
    "for sentence in [EAP_test_string, HPL_test_string, MWS_test_string]:\n",
    "    sample_vecs = []\n",
    "    for token in tokenizer.tokenize(sentence):\n",
    "        try:\n",
    "            sample_vecs.append(wv[token])\n",
    "        except KeyError:\n",
    "            # print(token, \"not in wv\")\n",
    "            pass\n",
    "    vectorized_data.append(sample_vecs)\n",
    "pickle.dump(vectorized_data, open(\"glove_vectorized_test_sentences\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = pickle.load(open(\"glove_vectorized_test_sentences\", \"rb\"))\n",
    "vectorized_data = pad_trunc(vectorized_data, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 300)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.asarray(vectorized_data)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_test = np.reshape(test, (len(test), maxlen, embedding_dims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 100, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = glove_keras_cnn()\n",
    "model = cnn.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (keras)",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
